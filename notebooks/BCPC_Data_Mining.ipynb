{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b726b0f",
   "metadata": {},
   "source": [
    "# Exploration of Data mining the BCPC database\n",
    "\n",
    "http://www.bcpcpesticidecompendium.org/index_cn_frame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b438be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "import requests\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_get_column_value_else_nan(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Get the value of the first row with the column set in the arguments. If not column doens't exist, return NaN.\n",
    "\n",
    "    When webscraping tables you sometimes stumble upon a table that's missing a certain column.\n",
    "    To avoid KeyErrors breaking your scrape loop, this function catches the KeyError and returns a NaN instead.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        value = df.loc[0, col_name]\n",
    "    except KeyError:\n",
    "        value = np.nan\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c5798",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f520c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb9fe8",
   "metadata": {},
   "source": [
    "### BCPC database\n",
    "http://www.bcpcpesticidecompendium.org/index_cn_frame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb65b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_database = \"http://www.bcpcpesticidecompendium.org/index_cn.html\"\n",
    "\n",
    "file_name = \"Pesticide_Database_BCPC\"\n",
    "file_location = \"../data/raw/\"\n",
    "ENABLE_FINISHED_BEEP = True\n",
    "\n",
    "date_today = datetime.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Load scraped database if it exists already\n",
    "if os.path.isfile(f\"{file_location}{file_name}\"):\n",
    "    df_pesticide = pd.read_excel(f\"{file_location}{date_today}_{file_name}.xlsx\")\n",
    "    print(\"Dataframe loaded\")\n",
    "# Else: scrape database\n",
    "else:\n",
    "    # Inialisation\n",
    "    pesticides = (\n",
    "        {}\n",
    "    )  # library with each element being a single pesticide with its respective properties\n",
    "    i_pest = (\n",
    "        0  # index of pesticide, used to move scraped info into the pesticides library\n",
    "    )\n",
    "    time_start = (\n",
    "        time.time()\n",
    "    )  # start time of the scraping => to display total scrape time\n",
    "    # Play beep after scrape finished:\n",
    "    duration = 750  # milliseconds\n",
    "    freq = 4 * 440  # Hz\n",
    "\n",
    "    # Beautiful soup the database\n",
    "    response = requests.get(url_database, headers=headers)\n",
    "    # read encoding from webpage to ensure special characters are scraped correctly, if missing: None\n",
    "    encoding = (\n",
    "        response.encoding\n",
    "        if \"charset\" in response.headers.get(\"content-type\", \"\").lower()\n",
    "        else None\n",
    "    )\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\", from_encoding=encoding)\n",
    "\n",
    "    # Pesticides are alphabetically grouped: loop over all 'p' tags.\n",
    "    # Note: there are 38 'p' tags, but only 27 contain 'a' tags == 26 letters + 1 numerical,\n",
    "    # the rest are empty and can be still looped over\n",
    "    groups = soup.find_all(\"p\")\n",
    "    n_groups = len(groups)\n",
    "\n",
    "    # Initialise progress display\n",
    "    disp1 = display(f\"BCPC Database Scrape has started:\", display_id=True)\n",
    "    disp2 = display(f\"Elapsed Time:\", display_id=True)\n",
    "\n",
    "    # Loop over all groups\n",
    "    for i_group in range(1, n_groups):  # skip first group as this contains the header\n",
    "        group = groups[i_group]\n",
    "        # Get pesticide items from this group\n",
    "        items = group.select(\"a\")\n",
    "        n_item = len(items)\n",
    "\n",
    "        if n_item > 0:  # if group contains no item => skip group\n",
    "            # the first letter of this group is used for displaying progress\n",
    "            group_letter = items[0].text[0].upper()\n",
    "\n",
    "            # Loop over all items in one group\n",
    "            for i_item, item in enumerate(items):\n",
    "                # Extract name and url from the item\n",
    "                name_pest = item.text\n",
    "                url_pest = url_database.replace(\"index_cn.html\", \"\") + item.get(\"href\")\n",
    "\n",
    "                # use pandas automatically extract tables from the webpage\n",
    "                df = pd.read_html(url_pest)[0]\n",
    "                df = df.set_index(0).transpose().reset_index(drop=True)\n",
    "\n",
    "                # Not all fields are always present: check them one by one and if missing => NaN\n",
    "                casname = try_get_column_value_else_nan(df=df, col_name=\"CAS name:\")\n",
    "                casrn = try_get_column_value_else_nan(df=df, col_name=\"CAS Reg. No.:\")\n",
    "                iupac = try_get_column_value_else_nan(df=df, col_name=\"IUPAC name:\")\n",
    "                activity = try_get_column_value_else_nan(df=df, col_name=\"Activity:\")\n",
    "                if pd.notnull(activity):\n",
    "                    # comma seperate string values\n",
    "                    activity = activity.replace(\") \", \"), \")\n",
    "\n",
    "                # Store pesticide info in library\n",
    "                pesticides[i_pest] = {\n",
    "                    \"Pesticide Common Name\": name_pest,\n",
    "                    \"url\": url_pest,\n",
    "                    \"CAS name\": casname,\n",
    "                    \"CAS RN\": casrn,\n",
    "                    \"IUPAC name:\": iupac,\n",
    "                    \"Activity:\": activity,\n",
    "                }\n",
    "                i_pest = i_pest + 1  # update pesticide index\n",
    "\n",
    "                # Update progress and time displays\n",
    "                time_elapsed = time.time() - time_start\n",
    "                disp1.update(\n",
    "                    f\"Group: {group_letter} - Page: {i_item+1}/{n_item} - Pesticides scraped: {i_pest}\"\n",
    "                )\n",
    "                disp2.update(f\"Time elapsed: {round(time_elapsed/60, 1)} min\")\n",
    "\n",
    "                time.sleep(0.3)  # wait between each scrape to avoid ban\n",
    "                del df  # to make sure no info from the previous iteration leaks into the next\n",
    "\n",
    "    # Dict to dataframe\n",
    "    df_pesticide = pd.DataFrame.from_dict(pesticides, \"index\")\n",
    "\n",
    "    # Export\n",
    "    #   Store dataframe so you don't have to scrape every time\n",
    "    df_pesticide.to_excel(f\"{file_location}{date_today}_{file_name}.xlsx\", index=False)\n",
    "    print(\"Dataframe saved\")\n",
    "\n",
    "    # Scrape timer\n",
    "    time_elapsed = time.time() - time_start\n",
    "    disp2.update(\n",
    "        f\"Total Time: {round(time_elapsed/60, 1)} min. (avg. {round(time_elapsed/i_pest, 1)} sec. per pesticide)\"\n",
    "    )\n",
    "\n",
    "    if ENABLE_FINISHED_BEEP:\n",
    "        winsound.Beep(freq, duration)\n",
    "\n",
    "\n",
    "# Print df info:\n",
    "print(df_pesticide.shape)\n",
    "df_pesticide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e17ef4",
   "metadata": {},
   "source": [
    "### Save requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378da5f-45ee-4c69-ae85-d9ce03d9fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipreqsnb --use-local --encoding=iso-8859-1 --ignore .venv --force ..\n",
    "# --use-local ONLY local package info instead of querying PyPI\n",
    "# --encoding=iso-8859-1 to avoid encoding errors related to utf-8\n",
    "# --force overwrites current file\n",
    "# .. saves it in parent folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
